{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden Markov Model:\n",
    "实验的比较：\n",
    "\n",
    "（1）原文(Isabel Kreis):        \n",
    "stimuli:呈现垂直光栅（等同于凝视点）后，出现左或右两个朝向的gabor    \n",
    "被试resp：预测下一次会出现左还是右的gabor  \n",
    "feedback:在下一个试次中以stimulus呈现\n",
    "\n",
    "（2）windows-animals实验  \n",
    "stimulus--不同颜色的window  \n",
    "被试resp：预测背后的动物  \n",
    "feedback：直接反馈对或错\n",
    "\n",
    "如果让两个实验一致，可以理解为在w-a实验中，呈现的stimulus为window-animal 的联结，被试resp：预测下一次会出现什么联结（association）。依照原文的实验，把stimuli也就是本实验中的asso分为两类:\n",
    "\n",
    "1-----黄色--猫/蓝色--狗-------原文中的left朝向\n",
    "\n",
    "2-----蓝色--猫/黄色--狗---------------right朝向"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------这是一条分界线------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分是导入数据，但是应该把原先的u_t换成实际的feedback（）和选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  This program run nback model\n",
    "import numpy as np\n",
    "import stan\n",
    "import pickle\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = np.loadtxt(\"demo_files/action.txt\", dtype=int)\n",
    "rewards = np.loadtxt(\"demo_files/rewards.txt\", dtype=int)\n",
    "T = 320\n",
    "mini = np.finfo(float).eps\n",
    "# pack the data(但这里的data应该转变)\n",
    "data_dict = {\n",
    "    'T'  : T,\n",
    "    'mini':mini,\n",
    "    'action' :list(action),#这里应该把u_t/resp改为reward/action,其中action对应的就是category 的联结，reward即±1\n",
    "    'resp':list(rewards)\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写stan model的思路"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先写model部分\n",
    "参数（需要被采样的）：\n",
    "      γ（gamma）：transition probability（0~1）\n",
    "      c: reward sensitivity （0.5~1）\n",
    "      d：punishment sensitivity（0.5~1）\n",
    "\n",
    "写生成关于action的预测：（update function）\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prior of P(St|St-1):\n",
    "P(St|St-1) = |1-gamma    gamma|\n",
    "             |gamma    1-gamma|\n",
    "             一共只有两个state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "P(St|St-1):follow transition matrix(每个trial之间独立，相当于2states-2actions)\n",
    "            S1          s2\n",
    "    S1      1-gamma     gamma\n",
    "    S2      gamma       1-gamma\n",
    "    \n",
    "Ps[1] = (1-gamma) * Ps[1] + gamma * Ps[2]\n",
    "Ps[2] = 1-Ps[1]\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了obs之后产生action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action selection based on the State probability \n",
    "'''choice[t] ~ categorical(Ps)'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但以上是latent states ，被试还会因为对reward/punishment的敏感程度不同（c/d）获得观察结果的概率（可以理解为s---（measurement noise）-----x？）\n",
    "the probability of actually observing this outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "P(Ot|Ot)/0.5:\n",
    "            S1          S2\n",
    "    O1      c           1-c\n",
    "    O2      1-d         d\n",
    "P0[1] = 0.5 * (c*St[1] +(1-c)*St[2])\n",
    "PO[2] = 0.5 * ((1-d)*St[1] + d*St[2])  \n",
    "\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在获得outcome之后，St的posterior prob就会根据P（St）和outcome进行合并更新\n",
    "公式为：P(St) = P(Ot|St)*P(St)/((ΣSt P(Ot|St)*P(St)))\n",
    "    \n",
    "    \n",
    "    P1 + P2 = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合并以上内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm='''\n",
    "data {\n",
    "    real mini; // to avoid p = 0 or 1-p=0,\n",
    "  \n",
    "    int<lower=0> T; // 试次数量\n",
    "  \n",
    "    array[T] int<lower=-1, upper=1> reward; // \n",
    "    array[T] int<lower=1, upper=2> action; // \n",
    "\n",
    "transformed data{\n",
    "  vector[2] init_Ps; // initial prob/belief of the two states\n",
    "  init_Ps = rep_vector(0.5, 2);\n",
    "}\n",
    "\n",
    "parameters {\n",
    "    real <lower=0,upper=1> gamma_pr; \n",
    "    real <lower=0.5,upper=1> c_pr; \n",
    "    real <lower=0.5,upper=1> d_pr;\n",
    "}\n",
    "\n",
    "transformed parameters{\n",
    "    real gamma;\n",
    "    real c;\n",
    "    real d;\n",
    "    vector[2] Ps;  // prob of the states, 1 - yellow-cat(blue-dog), 2 - yellow-dog(blue-cat)\n",
    "    real P_O_S1;   // p(O|S1) - O = {A,R} given yellow-cat\n",
    "    real P_O_S2;   // p(O|S2) - O = {A,R} given yellow-dog\n",
    "  \n",
    "    real gamma = Phi_approx(gamma_pr);//将先验参数进行转换和归一化,以确保其满足所需的范围和约束条件。将输入值通过逆标准正态累积分布函数进行转换。范围（0-1）\n",
    "    real c = Phi_approx(c_pr) * 0.5 + 0.5;//范围(0.5-1)\n",
    "    real d = Phi_approx(d_pr) * 0.5 + 0.5;//范围(0.5-1)\n",
    "    \n",
    "    //*******************compute ******************************\n",
    "    for (t in 1:T)  {  \n",
    "    // 使用 transition matrix 进行State update\n",
    "    // S[t-1] -- S[t],obs之前\n",
    "    if (t == 1) {\n",
    "      Ps = init_Ps;\n",
    "    } \n",
    "    else {\n",
    "      Ps[1] = Ps[1] * (1 - gamma) + Ps[2] * gamma;\n",
    "      Ps[2] = 1 - Ps[1];\n",
    "    }\n",
    "\n",
    "    // action selection based on the State probability \n",
    "    choice[t] ~ categorical(Ps);\n",
    "      \n",
    "    // renew of emission prob: p(O|S1) p(O|S2); O 为 A(ction) 和 R(eward)的组合\n",
    "    // --> the probability of actually observing this outcome\n",
    "    if (reward[t] == 1) {\n",
    "      P_O_S1 = 0.5 * ((choice[t] == 1) ? c : (1 - c));\n",
    "      P_O_S2 = 0.5 * ((choice[t] == 2) ? c : (1 - c));\n",
    "    } else if (reward[t] == -1) {\n",
    "      P_O_S1 = 0.5 * ((choice[t] == 1) ? (1 - d) : d);\n",
    "      P_O_S2 = 0.5 * ((choice[t] == 2) ? (1 - d) : d);\n",
    "    }\n",
    "\n",
    "    // State belief update \n",
    "    Ps[1] = (P_O_S1 * Ps[1]) / (P_O_S1 * Ps[1] + P_O_S2 * Ps[2]);\n",
    "    Ps[2] = 1 - Ps[1];\n",
    "    } \n",
    "  }\n",
    "}\n",
    "}\n",
    "\n",
    "model {  \n",
    "  // prior of  parameters\n",
    "  gamma_pr ~ normal(0, 1);\n",
    "  c_pr ~ normal(0.5, 1);\n",
    "  d_pr ~ normal(0.5, 1);\n",
    "  \n",
    "  for (t in 1:T){\n",
    "    action[t] ~ bernoulli(choice[t]*0.999+mini);\n",
    "  }\n",
    "  \n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = stan.build(hmm, data=data_dict)  # specify the model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nChain = 4  # How Many Chains, depend on the # of cores of your computer. 4 seems good\n",
    "nWarmup = 10000  # How Many Burn-in Samples?\n",
    "nIter = 10000  # How Many Recorded Samples, including nWarmup samples\n",
    "save = True # whether save the fitting result or not\n",
    "watch_list = ['gamma_pr', 'c_pr', 'd_pr']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
