{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "import scipy\n",
    "import time\n",
    "\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = sum(map(ord, \"RL_PyMC\"))\n",
    "rng = np.random.default_rng(seed)\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "运行时间： 0.0 秒\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "def generate_data(rng, alpha, beta, n=100, p_r=None):\n",
    "    if p_r is None:\n",
    "        p_r = [0.4, 0.6]\n",
    "    actions = np.zeros(n, dtype=\"int\")\n",
    "    rewards = np.zeros(n, dtype=\"int\")\n",
    "    Qs = np.zeros((n, 2))\n",
    "\n",
    "    # Initialize Q table\n",
    "    Q = np.array([0.5, 0.5])\n",
    "    for i in range(n):\n",
    "        # Apply the Softmax transformation\n",
    "        exp_Q = np.exp(beta * Q)\n",
    "        prob_a = exp_Q / np.sum(exp_Q)\n",
    "\n",
    "        # Simulate choice and reward\n",
    "        a = rng.choice([0, 1], p=prob_a)\n",
    "        r = rng.random() < p_r[a]\n",
    "\n",
    "        # Update Q table\n",
    "        Q[a] = Q[a] + alpha * (r - Q[a])\n",
    "\n",
    "        # Store values\n",
    "        actions[i] = a\n",
    "        rewards[i] = r\n",
    "        Qs[i] = Q.copy()\n",
    "\n",
    "    return actions, rewards, Qs\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"运行时间：\", execution_time, \"秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_alpha = 0.5\n",
    "true_beta = 5\n",
    "n = 150\n",
    "actions, rewards, Qs = generate_data(rng, true_alpha, true_beta, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q(action, reward, Qs, alpha):\n",
    "    \"\"\"\n",
    "    This function updates the Q table according to the RL update rule.\n",
    "    It will be called by pytensor.scan to do so recursevely, given the observed data and the alpha parameter\n",
    "    This could have been replaced be the following lamba expression in the pytensor.scan fn argument:\n",
    "        fn=lamba action, reward, Qs, alpha: pt.set_subtensor(Qs[action], Qs[action] + alpha * (reward - Qs[action]))\n",
    "    \"\"\"\n",
    "\n",
    "    Qs = pt.set_subtensor(Qs[action], Qs[action] + alpha * (reward - Qs[action]))\n",
    "    return Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def right_action_probs(alpha, beta, actions, rewards):\n",
    "    rewards = pt.as_tensor_variable(rewards, dtype=\"int32\")\n",
    "    actions = pt.as_tensor_variable(actions, dtype=\"int32\")\n",
    "\n",
    "    # Compute the Qs values\n",
    "    Qs = 0.5 * pt.ones((2,), dtype=\"float64\")\n",
    "    Qs, updates = pytensor.scan(\n",
    "        fn=update_Q, sequences=[actions, rewards], outputs_info=[Qs], non_sequences=[alpha]\n",
    "    )\n",
    "\n",
    "    # Apply the sotfmax transformation\n",
    "    Qs = Qs[:-1] * beta\n",
    "   \n",
    "    logp_actions = Qs - pt.logsumexp(Qs, axis=1, keepdims=True)\n",
    "\n",
    "    # Return the probabilities for the right action, in the original scale\n",
    "    return pt.exp(logp_actions[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [alpha, beta]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 46 seconds.\n"
     ]
    }
   ],
   "source": [
    "with pm.Model() as m_alt:\n",
    "    alpha = pm.Beta(name=\"alpha\", alpha=1, beta=1)\n",
    "    beta = pm.HalfNormal(name=\"beta\", sigma=10)\n",
    "\n",
    "    action_probs = right_action_probs(alpha, beta, actions, rewards)\n",
    "    like = pm.Bernoulli(name=\"like\", p=action_probs, observed=actions[1:])\n",
    "\n",
    "    tr_alt = pm.sample(random_seed=rng)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
