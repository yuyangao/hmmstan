{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  This program run nback model\n",
    "import numpy as np\n",
    "import stan\n",
    "import pickle\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_t = np.loadtxt(\"demo_files/u_t.txt\", dtype=int)\n",
    "resp = np.loadtxt(\"demo_files/resp.txt\", dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrial = 320\n",
    "# pack the data(但这里的data应该转变)\n",
    "data_dict = {\n",
    "    'nTrial': nTrial,\n",
    "    'u_t':list(u_t),\n",
    "    'resp':list(resp)\n",
    "}\n",
    "\n",
    "hmm='''\n",
    "data {\n",
    "  int<lower=1> S; //subject\n",
    "  int<lower=1> T; // num trials \n",
    "  int<lower=-1, upper=1> reward[S,T]; // reward set to -1 and 1\n",
    "  int<lower=1, upper=2> choice[S,T]; // originally 0=left, 1=right \n",
    "}\n",
    "\n",
    "transformed data {\n",
    "  vector[2] initQ;  // initial values for Q\n",
    "  initQ = rep_vector(0.0, 2);\n",
    "}\n",
    "\n",
    "parameters {\n",
    "  // add group level parameter names:\n",
    "  real beta_mu_pr;\n",
    "  real<lower=0> beta_sd;\n",
    "  vector[S] beta_pr; \n",
    "}\n",
    "\n",
    "transformed parameters{\n",
    "  // subject level parameters\n",
    "  vector <lower=0,upper=20>[S] beta; // inverse temp.\n",
    "  beta=Phi_approx(beta_mu_pr+beta_sd*beta_pr)*20;\n",
    "}\n",
    "\n",
    "model {  \n",
    "  beta_mu_pr~normal(0,1);\n",
    "  beta_sd~cauchy(0,3);\n",
    "  beta_pr~normal(0,1);\n",
    "  \n",
    "  {\n",
    "    for (s in 1:S){\n",
    "      vector[2] Q;  // Q value for all actions\n",
    "      Q = initQ;\n",
    "      \n",
    "      for (t in 1:T)  {\n",
    "        choice[s,t] ~ categorical_logit( beta[s]*Q ); // \"sampling statement\"\"\n",
    "        \n",
    "        Q[choice[s,t]]   = (reward[s,t] == 1)?1:(-1);\n",
    "        Q[3-choice[s,t]] = (reward[s,t] == 1)?(-1):1;\n",
    "      } // end of t loop (T trials)\n",
    "    } // end of s loop (S subjects)\n",
    "  }\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "  real <lower=0,upper=20> beta_mu;\n",
    "  real log_lik[S];\n",
    "  int  y_pred[S, T];\n",
    "  \n",
    "  beta_mu=Phi_approx(beta_mu_pr)*20;\n",
    "  y_pred = rep_array(-999,S ,T);\n",
    "  \n",
    "  {\n",
    "    for (s in 1:S){\n",
    "      vector[2] Q;  // Q value for all actions\n",
    "      Q = initQ;\n",
    "      log_lik[s]=0;\n",
    "      \n",
    "      for (t in 1:T)  {\n",
    "        log_lik[s] += categorical_logit_lpmf(choice[s,t] | beta[s]*Q );\n",
    "        y_pred[s,t] = categorical_logit_rng( beta[s] * Q ); \n",
    "        \n",
    "        Q[choice[s,t]]   = (reward[s,t] == 1)?1:(-1);\n",
    "        Q[3-choice[s,t]] = (reward[s,t] == 1)?(-1):1;\n",
    "      } // end of t loop (T trials)\n",
    "    } // end of t loop (S subjects)\n",
    "  }\n",
    "}  \n",
    "\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
